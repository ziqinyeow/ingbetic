{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "452ee8e2-7d0f-4a52-97e6-7976bb21e4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.2 which is incompatible.\n",
      "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U transformers huggingface_hub datasets accelerate peft onnx onnxruntime optimum\n",
    "# !pip install -q transformers==4.28.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cabae93-a56e-48e6-bf6e-36f65b15700b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected operating system as Ubuntu/focal.\n",
      "Checking for curl...\n",
      "Detected curl...\n",
      "Checking for gpg...\n",
      "Detected gpg...\n",
      "Detected apt version as 2.0.9\n",
      "Running apt-get update... done.\n",
      "Installing apt-transport-https... done.\n",
      "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
      "Importing packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\n",
      "done.\n",
      "Running apt-get update... done.\n",
      "\n",
      "The repository is setup! You can now install packages.\n"
     ]
    }
   ],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6428ceeb-9fd1-455d-9cb7-f4d4102d6d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  git-lfs\n",
      "0 upgraded, 1 newly installed, 0 to remove and 111 not upgraded.\n",
      "Need to get 7419 kB of archives.\n",
      "After this operation, 16.0 MB of additional disk space will be used.\n",
      "Get:1 https://packagecloud.io/github/git-lfs/ubuntu focal/main amd64 git-lfs amd64 3.3.0 [7419 kB]\n",
      "Fetched 7419 kB in 1s (11.9 MB/s)\n",
      "Selecting previously unselected package git-lfs.\n",
      "(Reading database ... 69943 files and directories currently installed.)\n",
      "Preparing to unpack .../git-lfs_3.3.0_amd64.deb ...\n",
      "Unpacking git-lfs (3.3.0) ...\n",
      "Setting up git-lfs (3.3.0) ...\n",
      "Git LFS initialized.\n",
      "Processing triggers for man-db (2.9.1-1) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d522266b-71f2-4e92-a187-dfb8e497bd0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a509e74171a046dea452a1c3281c17d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c040efa-11e3-49de-b1e0-1feb5a54bdc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\", \n",
    "    num_labels=1, \n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891d93c1-5b45-4b52-991c-5f00405ab123",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/ziqin/.cache/huggingface/datasets/ziq___parquet/ziq--ingredient_to_sugar_level-abc8bf7a5ca03ced/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbee45f6a2a04b4d9b4f1b37d78a6321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/ziqin/.cache/huggingface/datasets/ziq___parquet/ziq--ingredient_to_sugar_level-abc8bf7a5ca03ced/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-5ab1971a06811368.arrow\n",
      "Loading cached processed dataset at /Users/ziqin/.cache/huggingface/datasets/ziq___parquet/ziq--ingredient_to_sugar_level-abc8bf7a5ca03ced/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d480b10eb64c2663.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 22592\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1965\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('ziq/ingredient_to_sugar_level')\n",
    "dataset = dataset.rename_column(\"ingredients\", \"text\").rename_column(\"sugar\", \"labels\").remove_columns(\"src\")\n",
    "\n",
    "dataset = dataset.map(lambda x: tokenizer(x['text'], padding='max_length', truncation=True))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eacb424f-fd15-49ec-b96a-fa766c1734d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ziq___parquet/ziq--ingredient_to_sugar_level-abc8bf7a5ca03ced/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-de7cab93b2c7c9b4.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ziq___parquet/ziq--ingredient_to_sugar_level-abc8bf7a5ca03ced/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-1f621f5d524f2847.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 22592\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 1965\n",
       " }))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset['train'].shuffle(seed=42)\n",
    "test_dataset = dataset['test'].shuffle(seed=42)\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "898594f3-7812-4a05-8ca0-dacffdf60fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "MICRO_BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 3e-4\n",
    "TRAIN_STEPS = 2000\n",
    "OUTPUT_DIR = \"ingbetic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29053537-1ab3-4201-9ff9-7a7637fa813e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/notebooks/ingbetic is already a clone of https://huggingface.co/ziq/ingbetic. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        OUTPUT_DIR, \n",
    "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=100,\n",
    "        max_steps=TRAIN_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_steps=50,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669b84b6-81c8-45ee-9dd2-017fb165bb04",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6fbfb-1e36-44dc-8a3b-fba060eeca5b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train(\"ingbetic/checkpoint-1500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e779574b-082f-4ac5-956e-33c174b1a8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972f5a97840c41b1a78506ac1da37bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7292558f8b3248ed982662fbf1f261f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Jun07_18-30-10_ny736lt2rd/events.out.tfevents.1686162626.ny736lt2rd.9572.0:   0%|          | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/ziq/ingbetic\n",
      "   a5eb3e7..4708bbb  main -> main\n",
      "\n",
      "To https://huggingface.co/ziq/ingbetic\n",
      "   4708bbb..60450a0  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/ziq/ingbetic/commit/4708bbb8b6ee82ff9514601c63b487ad9c8d93b5'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f4a4763-00bd-4153-9807-77b73574ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ziq/ingbetic\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f68d53a6-674d-4355-94a9-311346a41fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = tokenizer(\"1 pound ground beef, 1 tablespoon dehydrated onion, 1 teaspoon salt, 1/2 teaspoon pepper, 1 teaspoon garlic powder, 1/4 teaspoon oregano, 1 cup uncooked penne pasta, 1 cup water, 1 (15-ounce) can fire-roasted diced tomatoes, 1/4 cup Parmesan cheese, 1/2 cup shredded mozzarella cheese, 4-6 fresh basil leaves, torn\", padding='max_length', truncation=True)\n",
    "input_ids = torch.tensor(dummy['input_ids']).unsqueeze(0)\n",
    "attention_mask = torch.tensor(dummy['attention_mask']).unsqueeze(0)\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd6c2ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = torch.tensor([dummy['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56c9a67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[2.7034]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df86f5be-2895-4224-8359-84b21aa48529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4866]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids=input_ids, attention_mask=attention_mask)['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237a2568-eb49-4b64-9d30-3dbb2e81f43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\"text-classification\",model=model,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "920f7581-f6ff-450f-a400-b4b76fad8471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.38068893551826477}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"1 pound ground beef, 1 tablespoon dehydrated onion, 1 teaspoon salt, 1/2 teaspoon pepper, 1 teaspoon garlic powder, 1/4 teaspoon oregano, 1 cup uncooked penne pasta, 1 cup water, 1 (15-ounce) can fire-roasted diced tomatoes, 1/4 cup Parmesan cheese, 1/2 cup shredded mozzarella cheese, 4-6 fresh basil leaves, torn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad7f824b-d599-47f0-bb30-ffd1049d973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model, \n",
    "    tuple((input_ids, attention_mask)),\n",
    "    f=\"sugar.onnx\",  \n",
    "    input_names=['input_ids', 'attention_mask'], \n",
    "    output_names=['logits'], \n",
    "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'}, \n",
    "                  'attention_mask': {0: 'batch_size', 1: 'sequence'}, \n",
    "                  'logits': {0: 'batch_size', 1: 'sequence'}}, \n",
    "    do_constant_folding=True, \n",
    "    opset_version=13, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d23e2686-597a-4507-8bee-c53fa3854ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.0/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.0/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.1/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.1/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.2/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.2/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.3/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.3/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.4/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.4/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.5/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.5/attention/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "quantize_dynamic(\"sugar.onnx\", \"sugar-int8.onnx\", \n",
    "                 weight_type=QuantType.QUInt8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a05890f-0e80-49d6-a14f-10987db30d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "\n",
    "# # load model from hub and convert\n",
    "# model = ORTModelForSequenceClassification.from_pretrained(\"ziq/ingbetic\",from_transformers=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# # save converted model\n",
    "# model.save_pretrained(\"ingbetic\")\n",
    "# tokenizer.save_pretrained(\"ingbetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d265c9fb-9455-46c7-afb3-0082c0403b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(\"sugar.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d43a59-0154-4be0-aae0-9f5e5cae94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "ort_sess = ort.InferenceSession('model-int8.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ad2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = dummy['input_ids']\n",
    "attention_mask = dummy['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27e582de-9e07-4554-8269-890d245e17a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.47709924]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = ort_sess.run(None, {'input_ids': input_ids.numpy(), 'attention_mask': attention_mask.numpy()})\n",
    "\n",
    "# Print Result \n",
    "outputs[0] #* 13.3627190349059 + 10.85810766787474"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f3519-fa8a-4bb2-bf89-c270b1b5beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "# api.upload_file(\n",
    "#     path_or_fileobj=\"sugar.onnx\",\n",
    "#     path_in_repo=\"model.onnx\",\n",
    "#     repo_id=\"ziq/ingbetic\",\n",
    "#     repo_type=\"model\",\n",
    "# )\n",
    "\n",
    "# api.upload_file(\n",
    "#     path_or_fileobj=\"sugar-int8.onnx\",\n",
    "#     path_in_repo=\"model-int8.onnx\",\n",
    "#     repo_id=\"ziq/ingbetic\",\n",
    "#     repo_type=\"model\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c043e57e-3a2f-4440-b774-06f1914891a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj=\"main.ipynb\",\n",
    "    path_in_repo=\"train.ipynb\",\n",
    "    repo_id=\"ziq/ingbetic\",\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7ec74-acde-4e22-a4cb-d44fcf1fd029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
